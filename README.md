# CodeGuard â€” LLM Bug Taxonomy Classifier & Analyzer

> **Version:** 0.0.7 &nbsp;|&nbsp; **Backend:** FastAPI on Render &nbsp;|&nbsp; **Extension:** VS Code Marketplace &nbsp;|&nbsp; **Date:** February 2026

---

## ğŸ“„ Abstract

Large Language Models (LLMs) such as GitHub Copilot and ChatGPT have become primary sources of code in modern software development. While these tools accelerate development, they introduce a class of bugs that existing static analysis tools are fundamentally unprepared to detect â€” bugs rooted not in syntax errors but in **semantic misalignment between the developer's intent (the prompt) and the generated code**. CodeGuard is a three-stage hybrid static-dynamic-linguistic bug detection system designed specifically to identify LLM-generated bug patterns. It defines a novel taxonomy of 10 LLM-specific bug patterns, runs a three-stage analysis pipeline (AST-based static analysis, Docker-sandboxed dynamic execution, and intent-aware linguistic analysis), and delivers results directly inside VS Code via a sidebar panel. Evaluated across 160 test cases, CodeGuard achieves **73.12% accuracy**, **87.50% recall**, and an **F1 score of 76.50%**, demonstrating that runtime + linguistic signals together capture bugs that static analysis alone misses.

---

## ğŸ“‘ Table of Contents

1. [Introduction](#1-introduction)
2. [Literature Review](#2-literature-review)
3. [Bug Taxonomy](#3-bug-taxonomy)
4. [System Architecture](#4-system-architecture)
5. [Applied Methodology](#5-applied-methodology)
6. [Tool Demonstration](#6-tool-demonstration)
7. [Test Plan & Results](#7-test-plan--results)
8. [Limitations & Future Work](#8-limitations--future-work)
9. [Conclusion](#9-conclusion)
10. [References](#10-references)
11. [Appendix](#11-appendix)

---

## List of Abbreviations

| Abbreviation | Meaning |
|---|---|
| LLM | Large Language Model |
| NPC | Non-Prompted Consideration |
| AST | Abstract Syntax Tree |
| TP | True Positive |
| TN | True Negative |
| FP | False Positive |
| FN | False Negative |
| F1 | F1 Score (harmonic mean of Precision and Recall) |
| API | Application Programming Interface |
| IDE | Integrated Development Environment |
| TF-IDF | Term Frequencyâ€“Inverse Document Frequency |

---

## 1. Introduction

### 1.1 Background & Motivation

The adoption of LLM-based code generation tools has grown exponentially since the release of GitHub Copilot in 2021. Developers routinely accept AI-generated code with minimal review, trusting that tools such as ChatGPT, Copilot, and CodeLlama produce correct, production-ready implementations. Research consistently shows that LLM-generated code contains bugs in 25â€“40% of cases, yet developer review rates of AI suggestions remain low.

The problem is compounded by the nature of LLM bugs. Unlike traditional programming errors, LLM bugs frequently arise from:
- The model **hallucinating** class names, methods, or modules that do not exist
- The model generating code that is **correct for a related but different task** (prompt misinterpretation)
- The model **over-fitting to examples in the prompt** (prompt-biased code)
- The model correctly implementing the function body but **missing edge cases** the developer expected

These patterns are invisible to traditional linters (Pylint, Flake8) because such tools have no awareness of what the developer intended.

### 1.2 Problem Statement

> *"Existing static analysis tools such as Pylint and Flake8 are designed to detect general programming errors and style violations. They are not designed to detect LLM-specific bug patterns such as Hallucinated Objects, Prompt-Biased Code, or Misinterpretation of developer intent. No systematic taxonomy or detection tool exists for these failure modes."*

### 1.3 Objectives

1. Define a **taxonomy of 10 LLM-generated bug patterns** covering static, dynamic, and linguistic failure modes
2. Build a **three-stage hybrid detection pipeline**: static (AST), dynamic (Docker sandbox), and linguistic (intent analysis)
3. Integrate detection into the **developer workflow** as a VS Code extension with a real-time sidebar panel
4. Evaluate with **Precision, Recall, F1 Score, and Accuracy** across 160 labeled test cases

### 1.4 Scope

- Supports **Python only** (current version)
- Detects **10 specific bug patterns** mapped to a validated taxonomy
- Runs as a **VS Code extension** communicating with a cloud backend (Render.com)
- Linguistic stage uses an external **LLM API** (Ollama / OpenRouter) for semantic verdict

### 1.5 Report Structure

Chapter 2 reviews existing tools and the research gap. Chapter 3 defines the 10-pattern taxonomy. Chapter 4 presents the full system architecture. Chapter 5 details the methodology of each analysis stage. Chapter 6 demonstrates the tool. Chapter 7 presents test results. Chapter 8 discusses limitations. Chapter 9 concludes.

---

## 2. Literature Review

### 2.1 LLM Code Generation â€” Current State

GPT-4, GitHub Copilot, and CodeLlama represent the state of the art in neural code synthesis. Studies indicate that:
- Copilot suggestions are accepted 26â€“35% of the time (GitHub, 2023)
- 40% of Copilot-generated security-sensitive code contains vulnerabilities (Pearce et al., 2022)
- LLMs produce syntactically correct but semantically wrong code in approximately 30% of non-trivial tasks (Liu et al., 2023)

The core issue is **intent gap**: the model generates code that satisfies the surface form of the prompt without capturing deep semantic intent.

### 2.2 Existing Bug Detection Tools

| Tool | Type | Primary Use | Key Limitation |
|:--|:--|:--|:--|
| Pylint | Static | General Python linting | No semantic / intent analysis |
| Flake8 | Static | Style and PEP 8 | Style only; no logic errors |
| Bandit | Static | Security scanning | No LLM-specific patterns |
| SonarQube | Static/Dynamic | Enterprise code quality | Not LLM-aware |
| mypy | Static | Type checking | Requires type annotations |
| Semgrep | Static | Pattern matching | Manual rule authoring |

### 2.3 Research Gap

None of the above tools can:
- Detect a **prompt-code semantic mismatch** (the code works but solves the wrong problem)
- Identify **hallucinated references** at the semantic level (not just undefined names)
- Flag **non-prompted considerations** (features the LLM added that the developer did not request)

CodeGuard is the first tool to define and operationalize a taxonomy specifically targeting these LLM-generated failure modes.

---

## 3. Bug Taxonomy

### 3.1 Taxonomy Design Philosophy

The ten patterns were identified through a combination of:
1. Empirical analysis of >500 LLM-generated code samples from ChatGPT and Copilot
2. Classification of runtime error types produced during dynamic execution
3. Semantic gap analysis between prompt text and generated code structure

Each pattern is assigned a detection stage (static / dynamic / linguistic) reflecting the earliest and most reliable detection mechanism.

### 3.2 The 10 Bug Patterns

#### Pattern 1 â€” Syntax Error
- **Definition:** The generated code cannot be parsed by the Python interpreter.
- **Example:** `if not names` (missing colon) â€” observed in production Render logs.
- **Why LLMs generate this:** Token-level generation can produce incomplete control flow structures, especially near token limits.
- **Detection Stage:** Static (AST parse)
- **Severity Range:** 8â€“10

#### Pattern 2 â€” Hallucinated Object
- **Definition:** The code references a class, module, or function that does not exist in any imported namespace.
- **Example:** `result = calculator.compute(5)` where `calculator` was never defined or imported.
- **Why LLMs generate this:** The model has seen similar APIs in training data and generates plausible-but-fictional API calls.
- **Detection Stage:** Static + Dynamic (NameError confirmation)
- **Severity Range:** 7â€“9

#### Pattern 3 â€” Incomplete Generation
- **Definition:** The code contains `TODO`, `pass`, `...`, or truncated assignments indicating the LLM stopped mid-generation.
- **Example:** `def process(data): pass  # TODO: implement`
- **Why LLMs generate this:** Token limits or uncertainty causes the model to emit placeholders instead of implementations.
- **Detection Stage:** Static (pattern matching on AST + text)
- **Severity Range:** 6â€“8

#### Pattern 4 â€” Silly Mistake
- **Definition:** Logically incorrect but syntactically valid code â€” identical if/else branches, reversed operands, off-by-one errors.
- **Example:** `if discount > 0: price = price - discount` vs `else: price = price - discount` (identical branches).
- **Why LLMs generate this:** Copy-paste style generation at the token level does not enforce logical consistency between branches.
- **Detection Stage:** Static (AST branch comparison)
- **Severity Range:** 5â€“7

#### Pattern 5 â€” Wrong Attribute
- **Definition:** Accessing an attribute using incorrect syntax for the data type (e.g., dot-access on a dictionary).
- **Example:** `user = {"name": "Alice"}; print(user.name)` â€” should be `user["name"]`.
- **Why LLMs generate this:** Conflation of object-style and dict-style access, common across languages.
- **Detection Stage:** Dynamic (AttributeError at runtime)
- **Severity Range:** 6â€“8

#### Pattern 6 â€” Wrong Input Type
- **Definition:** Passing an argument of the wrong type to a function or operator.
- **Example:** `result = "hello" + 5` â€” TypeError at runtime.
- **Why LLMs generate this:** Type information is often absent from prompts; the model infers types incorrectly from context.
- **Detection Stage:** Dynamic (TypeError at runtime)
- **Severity Range:** 5â€“7

#### Pattern 7 â€” Non-Prompted Consideration (NPC)
- **Definition:** The code implements features, safeguards, or patterns the developer never requested (e.g., `admin` checks, logging, caching).
- **Example:** Prompt asks for a simple sort function; the LLM generates a full CRUD API with authentication.
- **Why LLMs generate this:** Training data associates certain prompts with richer implementations; the model over-generates.
- **Detection Stage:** Linguistic (pattern matching + AST decorator analysis)
- **Severity Range:** 4â€“6

#### Pattern 8 â€” Prompt-Biased Code
- **Definition:** The code hardcodes example values from the prompt rather than implementing the general algorithm.
- **Example:** Prompt says "e.g., sort `[3, 1, 2]`"; the LLM hardcodes `return [1, 2, 3]`.
- **Why LLMs generate this:** In-context learning over-fits to prompt examples; the model returns the expected output for the given example instead of implementing the logic.
- **Detection Stage:** Linguistic (regex extraction of quoted values from prompt, cross-checked in code)
- **Severity Range:** 5â€“7

#### Pattern 9 â€” Missing Corner Case
- **Definition:** The code handles the happy path but omits critical edge cases (division by zero, empty input, None checks).
- **Example:** `def divide(a, b): return a / b` â€” no guard for `b == 0`.
- **Why LLMs generate this:** Single-path reasoning; the model generates the primary logic without reasoning about boundary conditions.
- **Detection Stage:** Static (pattern matching) + Dynamic (ZeroDivisionError at runtime)
- **Severity Range:** 4â€“6

#### Pattern 10 â€” Misinterpretation
- **Definition:** The code is technically correct but solves a different problem than what the developer intended (semantic mismatch).
- **Example:** Prompt requests a function that "filters and sorts"; the LLM generates a function that only sorts.
- **Why LLMs generate this:** Ambiguous prompts lead to partial implementations; the model anchors to the most prominent keyword.
- **Detection Stage:** Linguistic (TF-IDF cosine similarity between prompt keywords and code features)
- **Severity Range:** 6â€“9

### Pattern Summary Table

| # | Pattern | Detection Stage | Severity |
|:--|:--|:--|:--|
| 1 | Syntax Error | Static | 8â€“10 |
| 2 | Hallucinated Object | Static + Dynamic | 7â€“9 |
| 3 | Incomplete Generation | Static | 6â€“8 |
| 4 | Silly Mistake | Static | 5â€“7 |
| 5 | Wrong Attribute | Dynamic | 6â€“8 |
| 6 | Wrong Input Type | Dynamic | 5â€“7 |
| 7 | Non-Prompted Consideration (NPC) | Linguistic | 4â€“6 |
| 8 | Prompt-Biased Code | Linguistic | 5â€“7 |
| 9 | Missing Corner Case | Static + Dynamic | 4â€“6 |
| 10 | Misinterpretation | Linguistic | 6â€“9 |

---

## 4. System Architecture

### 4.1 High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              VS Code Extension (TypeScript)          â”‚
â”‚   Sidebar Panel â”‚ Prompt Input â”‚ Bug Highlights      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚  POST /api/analyze  (axios, 60s timeout)
                     â”‚  GET  /api/analysis/{id} (polling, 15s interval)
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FastAPI Backend (Python, Render.com)         â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              3-Stage Pipeline                 â”‚  â”‚
â”‚  â”‚                                              â”‚  â”‚
â”‚  â”‚  Stage 1          Stage 2         Stage 3   â”‚  â”‚
â”‚  â”‚  StaticAnalyzer   DynamicAnalyzer  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚  â”‚
â”‚  â”‚  (AST/astroid)    (Docker /        Linguisticâ”‚â”‚  â”‚
â”‚  â”‚                    subprocess)     Analyzer  â”‚â”‚  â”‚
â”‚  â”‚                                   [BG Task] â”‚â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                     â”‚                               â”‚
â”‚          TaxonomyClassifier + ExplainabilityLayer   â”‚
â”‚                     â”‚                               â”‚
â”‚          BackgroundTasks (linguistic, ~100s)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL / Supabase Database                    â”‚
â”‚   analyses â”‚ bug_patterns â”‚ execution_logs          â”‚
â”‚   linguistic_analyses â”‚ feedback                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key design decision â€” Background Task pattern:** The linguistic stage calls an external LLM API (Ollama/OpenRouter) and takes ~100 seconds. To avoid hitting Render's 60-second proxy timeout, the backend returns a preliminary response (`status: "processing"`) within ~2 seconds after completing stages 1 and 2. Stage 3 runs as a FastAPI `BackgroundTask`. The VS Code extension polls `GET /api/analysis/{id}` every 15 seconds until `status` changes to `"complete"`.

### 4.2 Technology Stack

| Layer | Technology | Version |
|:--|:--|:--|
| VS Code Extension | TypeScript, VS Code API, esbuild | 0.0.7 |
| HTTP Client (Extension) | axios (bundled via esbuild) | 1.13.4 |
| Backend Framework | FastAPI + uvicorn | 0.131.0 |
| Static Analysis | Python AST, pyflakes, astroid | astroid 4.0.4 |
| Dynamic Analysis | Docker SDK, `python:3.10-slim` image | docker 7.1.0 |
| Linguistic Analysis | spaCy, scikit-learn, TF-IDF | â€” |
| LLM API | Ollama Cloud (primary), OpenRouter (fallback) | ollama 0.6.1 |
| Database ORM | SQLAlchemy 2.0 | 2.0.46 |
| Database | PostgreSQL on Supabase | â€” |
| Rate Limiting | slowapi | 0.1.9 |
| Deployment | Render.com (free tier) | â€” |
| Python Version | 3.13.4 | â€” |

### 4.3 Database Schema

**`analyses`** â€” Primary record for each analysis request
| Column | Type | Description |
|---|---|---|
| `analysis_id` | Integer PK | Auto-increment identifier |
| `prompt` | Text | Developer's original prompt |
| `code` | Text | Code submitted for analysis |
| `language` | String(50) | Default: `'python'` |
| `overall_severity` | Integer | 0â€“10 composite severity |
| `has_bugs` | Boolean | True if any pattern detected |
| `summary` | Text | Human-readable summary |
| `confidence_score` | Float | 0.0â€“1.0 detection confidence |
| `created_at` | DateTime | UTC timestamp |

**`bug_patterns`** â€” One row per detected pattern per analysis
| Column | Type | Description |
|---|---|---|
| `bug_pattern_id` | Integer PK | â€” |
| `analysis_id` | FK â†’ analyses | Parent analysis |
| `pattern_name` | String(100) | One of the 10 taxonomy patterns |
| `severity` | Integer | 0â€“10 |
| `confidence` | Float | Detection confidence |
| `description` | Text | Explanation of the bug |
| `location` | String(255) | Line number / code location |
| `fix_suggestion` | Text | Recommended fix |
| `detection_stage` | String(50) | `'static'` / `'dynamic'` / `'linguistic'` |

**`execution_logs`** â€” Per-stage timing and error logs
| Column | Type | Description |
|---|---|---|
| `log_id` | Integer PK | â€” |
| `analysis_id` | FK â†’ analyses | â€” |
| `stage` | String(50) | `'static'` / `'dynamic'` / `'linguistic'` / `'classification'` |
| `success` | Boolean | â€” |
| `error_message` | Text | If failure |
| `execution_time` | Float | Seconds |

**`linguistic_analyses`** â€” Stage 3 results
| Column | Type | Description |
|---|---|---|
| `linguistic_id` | Integer PK | â€” |
| `analysis_id` | FK â†’ analyses | â€” |
| `intent_match_score` | Float | TF-IDF cosine similarity (0â€“1) |
| `unprompted_features` | Text | JSON list of NPC features |
| `missing_features` | Text | JSON list of missing keywords |
| `hardcoded_values` | Text | JSON list of prompt-biased values |

**`feedback`** â€” User ratings (one-to-many with analyses)
| Column | Type | Description |
|---|---|---|
| `id` | Integer PK | â€” |
| `analysis_id` | FK â†’ analyses | â€” |
| `rating` | Integer | 1â€“5 stars |
| `comment` | Text | Optional text comment |
| `is_helpful` | Boolean | â€” |

---

## 5. Applied Methodology

### 5.1 Workflow Overview

```
Input: { prompt: str, code: str }
       â”‚
       â–¼
Stage 1 â”€ StaticAnalyzer        (~10ms)
       â”‚    AST parse â†’ 10 checks
       â–¼
Stage 2 â”€ DynamicAnalyzer       (~200msâ€“5s)
       â”‚    Docker sandbox / subprocess fallback
       â–¼
Stage 3 â”€ TaxonomyClassifier    (~5ms)
       â”‚    Maps results to 10-pattern schema
       â–¼
Stage 4 â”€ ExplainabilityLayer   (~5ms)
       â”‚    Severity label + fix suggestions
       â–¼
Response: status="processing"   â†’ DB save (stages 1+2+4)
       â”‚
[BackgroundTask]
       â–¼
Stage 3b â”€ LinguisticAnalyzer   (~100s)
       â”‚    4 detectors Ã— LLM call
       â–¼
DB update: full patterns + status="complete"
```

### 5.2 Stage 1 â€” Static Analysis

#### 5.2.1 Syntax Error Detection
Uses `ast.parse()` (stdlib) and `astroid.parse()` (migrated detectors). Catches `SyntaxError` and `AstroidSyntaxError`, reports line number and column offset. The partial-parse fallback strips the offending line and continues analysis so other bug patterns can still be detected.

#### 5.2.2 Hallucinated Object Detection
Traverses the AST for `nodes.Name` (reads, not assignments). Cross-references against:
- Python built-ins whitelist (60+ names: `len`, `range`, `print`, `dict`, etc.)
- All names that appear as `nodes.AssignName` in the same scope
- All imported names from `import` and `from ... import` statements

Names that appear in a read context without any corresponding definition or import are flagged. Dynamic execution provides confirmation via `NameError` at runtime.

#### 5.2.3 Incomplete Generation Detection
Checks for:
- `pass` statements in non-abstract function bodies
- `TODO` / `FIXME` / `...` as the sole statement in a function
- Truncated assignments (`x =` with no right-hand side)
- Functions with empty bodies other than a docstring

#### 5.2.4 Silly Mistake Detection
Uses AST comparison to detect:
- Identical `if` and `else` branch bodies (dead branch)
- Reversed operands in financial calculations (`discount - price` instead of `price - discount`)
- Duplicate condition checks (`if x > 0 and x > 0`)

**Confidence score formula:**

$$\text{Confidence} = \frac{\text{Patterns Matched}}{\text{Total Checks}} \times 100$$

### 5.3 Stage 2 â€” Dynamic Analysis

#### 5.3.1 Docker Sandbox Architecture
When Docker Desktop is available (local development), code is executed inside an isolated container:
- **Base image:** `python:3.10-slim`
- **Memory limit:** 128 MB
- **CPU quota:** 50% of one core
- **Network:** disabled
- **Timeout:** 10 seconds (configurable via `DOCKER_TIMEOUT` env var)
- **Namespace isolation:** user code executes in a `_cg_ns = {}` dictionary namespace via `exec()` to prevent variable collisions with the wrapper harness

#### 5.3.2 Runtime Error Classification

| Runtime Error | Classified As |
|:--|:--|
| `AttributeError` | Wrong Attribute |
| `TypeError` | Wrong Input Type |
| `NameError` | Hallucinated Object (confirmed) |
| `ZeroDivisionError` | Missing Corner Case |
| `IndexError` / `KeyError` / `ValueError` | Other Error |
| Timeout | Execution timeout |

#### 5.3.3 Fallback Mechanism
On Render (no Docker daemon), the analyzer falls back to a subprocess execution with a safety check:
- Scans for dangerous imports (`os`, `subprocess`, `socket`, `threading`, etc.)
- Code containing these imports is skipped entirely
- All other code runs via `subprocess.run()` with a 5-second timeout
- This fallback is logged as a warning in Render logs

### 5.4 Stage 3 â€” Linguistic Analysis

#### 5.4.0 Overview â€” How It Works

The linguistic stage answers a question that neither AST parsing nor runtime execution can answer: **does the code actually do what the developer asked for?** It compares the *intent* expressed in the developer's natural-language prompt against the *behaviour* encoded in the generated code.

The stage is organized as four specialized detectors, each covering one semantic failure mode (NPC, Prompt Bias, Missing Features, Misinterpretation). Every detector applies the same **three-layer cascade** internally before producing its verdict:

```
  Input: { prompt, code }
          â”‚
          â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Each Detector (Ã—4)                                      â”‚
  â”‚                                                          â”‚
  â”‚  Layer 1 â€” Rule Engine         (~10ms)                   â”‚
  â”‚    Fast regex pattern matching                           â”‚
  â”‚    Returns: candidate evidence list                      â”‚
  â”‚          â”‚                                               â”‚
  â”‚          â–¼                                               â”‚
  â”‚  Layer 2 â€” AST Analyzer        (~50ms)                   â”‚
  â”‚    Structural verification using astroid                 â”‚
  â”‚    Returns: confirmed structural findings                â”‚
  â”‚          â”‚                                               â”‚
  â”‚          â–¼                                               â”‚
  â”‚  Layer 3 â€” LLM Reasoner        (~300ms per detector)     â”‚
  â”‚    Receives Layer 1 + Layer 2 evidence as context        â”‚
  â”‚    Makes final semantic verdict + severity score         â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
  Final result per detector: { found, issues, severity, explanation }
```

**Layer 1 â€” Rule Engine** is pure regex matching, completing in under 10 ms. It scans for catalogued patterns (NPC indicators, hardcoded literals, missing keywords, return-type markers) and assembles a preliminary evidence list. This layer has high recall but low precision â€” it flags many candidates that are not actual bugs.

**Layer 2 â€” AST Analyzer** uses astroid to perform structural verification. It confirms or discards the Layer 1 candidates by examining the actual parsed tree (`nodes.FunctionDef`, `nodes.Call`, `nodes.Const`, decorator lists, try-except blocks). astroid's `node.expr.infer()` provides semantic type inference which allows, for example, distinguishing a `dict` attribute access (`user["name"]`) from an object attribute access (`user.name`). Layer 2 provides near-100% structural accuracy at ~50 ms.

**Layer 3 â€” LLM Reasoner** receives the evidence collected by the first two layers as structured context, then makes the final semantic verdict. The LLM prompt includes the developer's original prompt, the generated code, and the bullet-point findings from Layers 1 and 2. The LLM evaluates whether the evidence constitutes a real intent mismatch and returns a natural-language explanation plus a severity score (0â€“10). This is the only layer capable of reasoning about *meaning*, not just structure.

---

#### 5.4.0.1 Why an LLM Instead of PyTorch / Transformer Libraries?

A natural question is: "why call an external LLM API instead of running a local model with PyTorch or HuggingFace Transformers?" Several factors make the external-API approach correct for this project:

| Criterion | PyTorch / transformers | External LLM API (Ollama / OpenRouter) |
|:--|:--|:--|
| **Deployment weight** | 500 MBâ€“2 GB model file | Zero local weight; zero RAM |
| **Render free tier RAM** | 512 MB total â€” model doesn't fit | No RAM consumed |
| **Cold-start time** | 20â€“60 s to load model into VRAM | <1 s API request |
| **GPU requirement** | NVIDIA GPU for inference speed | Inference runs on provider side |
| **Maintenance** | Re-train / fine-tune for each new pattern | Prompt-engineered; update a string |
| **Code complexity** | 200+ lines of tokenizer + inference pipeline | 10 lines HTTP request |
| **Reasoning quality on novel inputs** | Degrades without fine-tuning | GPT-scale models generalize well |

The semantic questions posed in Stage 3 ("does this code solve the problem described in the prompt?") require **general language understanding**, not domain-specific classification. This task type is exactly what instruction-tuned large models (GPT-class, Gemma 12B) excel at without fine-tuning â€” making them far more practical than training a 768-dimensional BERT model on a small labeled dataset (which would require thousands of labeled {prompt, code, verdict} examples that do not yet exist).

Additionally, the three-layer architecture deliberately minimizes LLM calls: Layers 1 and 2 filter out obvious non-bugs before the LLM ever sees them. The LLM only verdicts on cases where structural evidence is genuinely ambiguous â€” exactly the nuanced judgment task LLMs are built for. Rule-based layers handle the easy 80%; the LLM resolves the hard 20%.

**LLM API fallback chain:**
1. **Primary** â€” Ollama Cloud (`gpt-oss:20b-cloud`), authenticated via `OLLAMA_API_KEY`
2. **Fallback** â€” OpenRouter (`google/gemma-3-12b-it:free`), authenticated via `OPENROUTER_API_KEY`
3. **Disabled** â€” If both keys are absent, Layer 3 is skipped; Layers 1 and 2 results are returned directly

---

#### 5.4.1 NPC Detector

Each of the three layers targets NPC evidence specifically:

- **Layer 1 (Rule Engine):** Regex scan for a catalogue of NPC pattern groups â€” `debug_prints` (`print(`, `breakpoint()`), `logging` (`logger.`, `.debug(`, `.info(`), `validation` (`assert`, `raise`, `if not`), `error_handling` (`try:`, `except`), `auth` (`admin`, `permission`, `role`), `caching` (`@lru_cache`, `cache`, `lock`)
- **Layer 2 (AST Analyzer):** Confirms findings by checking whether matched patterns correspond to AST nodes actually present in the code; extracts decorator lists, try-except node presence, and function call names to eliminate false positives from string matches inside comments or docstrings
- **Layer 3 (LLM Reasoner):** Given the full prompt text, the code, and the Layer 1 + Layer 2 evidence, the LLM judges whether the flagged additions were truly unrequested â€” or whether they are implied by the prompt's domain context (e.g., a prompt about "production API" reasonably implies logging)

The Layer 3 verdict is what separates NPC detection from a naive keyword search.

#### 5.4.2 Prompt Bias Detector

- **Layer 1 (Rule Engine):** Regex extraction of quoted string literals and numeric constants from the prompt text; also scans for commonly hardcoded example names (`alice`, `bob`, `test@`, `user123`, `[3,1,2]`)
- **Layer 2 (AST Analyzer):** Walks `nodes.Const` nodes in the code and compares their values against the Layer 1 extracted literals; ignores constants inside `if __name__ == "__main__":` blocks (test harness exclusion) by checking the parent node's test condition
- **Layer 3 (LLM Reasoner):** Determines whether the literal matches represent true over-fitting (the code only works for the example values) or merely incidental appearance (a test, a docstring example, or a coincidence)

1. Extracts quoted string literals from the prompt using regex (`r'"([^"]+)"'` and single-quote variant)
2. Parses all `ast.Constant` nodes in the code
3. Flags code where prompt example values appear as hardcoded comparisons or return values
4. Ignores values inside `if __name__ == "__main__":` blocks (test harness exclusion)

#### 5.4.3 Missing Feature Detector

- **Layer 1 (Rule Engine):** Tokenizes the prompt using TF-IDF and builds a keyword set; tokenizes imported module names, function names, and variable identifiers from the code; computes the set difference (prompt keywords absent from code features)
- **Layer 2 (AST Analyzer):** Traverses `nodes.FunctionDef`, `nodes.Import`, `nodes.ImportFrom`, `nodes.Assign` to extract the structural vocabulary of the code; refines the Layer 1 keyword gap by eliminating synonyms and stop-words
- **Layer 3 (LLM Reasoner):** Evaluates whether the gap keywords represent features that *should* have been implemented (missing functionality) or are simply prompt phrasing not reflected in code identifiers (acceptable)

**Intent Match Score (TF-IDF Cosine Similarity):**

$$\text{similarity}(P, C) = \frac{\vec{P} \cdot \vec{C}}{\|\vec{P}\| \|\vec{C}\|}$$

where $\vec{P}$ is the TF-IDF vector of the prompt and $\vec{C}$ is the TF-IDF vector of the code identifiers. A score below ~0.40 indicates a significant semantic gap.

#### 5.4.4 Misinterpretation Detector

- **Layer 1 (Rule Engine):** Regex heuristics for structural mismatch signals â€” return type keywords in prompt ("list", "all", "every") vs. scalar return statements; intent verbs ("filter", "remove", "exclude") vs. iteration-only code; output intent ("display", "show", "print") vs. `return` statements
- **Layer 2 (AST Analyzer):** Confirms the structural hypothesis â€” checks `nodes.Return` value types, number of conditional branches, presence or absence of list comprehensions or filter expressions
- **Layer 3 (LLM Reasoner):** Performs the semantic judgment: given what the prompt asked for and what the code structure delivers, does the code solve a fundamentally different problem? The LLM rates the severity of the semantic mismatch.

The three checks below are the Layer 1 signals before LLM confirmation:
- Return type mismatch: prompt uses words like "list", "all", "every" but code returns a scalar
- Print vs return intent: prompt says "display" or "show" but code uses `return` (or vice versa)
- Filtering intent: prompt says "filter" but code only iterates without conditional exclusion

#### 5.4.5 LLM Verdict (Layer 3) â€” Summary

For each of the four detectors, the LLM Reasoner (`LLMReasoner.final_verdict()`) constructs a structured prompt containing:
1. The developer's original natural-language prompt
2. The generated code
3. Bullet-point evidence from Layer 1 (regex findings)
4. Bullet-point evidence from Layer 2 (AST findings)

The LLM (Ollama `gpt-oss:20b-cloud` or OpenRouter `gemma-3-12b-it:free`) returns a JSON verdict with `found` (boolean), `issues` (list), `severity` (0â€“10), and a natural-language explanation. This verdict is the final classification output for that detector.

Because the four LLM calls run sequentially (one per detector), and each call takes ~25 seconds at Ollama Cloud latency, the total Stage 3 duration is approximately **98â€“105 seconds**. This is why Stage 3 runs as a FastAPI `BackgroundTask` and the extension polls for completion.

### 5.5 Stage 4 â€” Taxonomy Classification

`TaxonomyClassifier` aggregates results from all three stages with the following priority:
1. **Static patterns** are always reported (syntax errors block dynamic/linguistic)
2. **Dynamic patterns** confirm or override static hypotheses (NameError confirms Hallucinated Object)
3. **Linguistic patterns** fill the semantic gap
4. If `len(bug_patterns) > 3`, a composite **Misinterpretation** pattern is added

### 5.6 Stage 5 â€” Explainability Layer

`ExplainabilityLayer` generates:
- **Severity label:** Critical (8â€“10) / High (6â€“7) / Medium (4â€“5) / Low (1â€“3)
- **Per-pattern description:** Human-readable explanation of why each pattern was detected
- **Fix suggestions:** Concrete code-level recommendations
- **Overall summary:** Single paragraph summarizing all detected issues

---

## 6. Tool Demonstration

### 6.1 VS Code Extension Interface

The extension contributes a sidebar panel registered under the `codeguard-sidebar` activity bar container. The webview panel (`codeguard.sidePanel`) renders `webview.html` via the `CodeGuardPanel` class implementing `vscode.WebviewViewProvider`.

**Interface elements:**
- **Prompt input** â€” Textarea for entering the original LLM prompt
- **Analyze button** â€” Triggers `analyzeCode` API call
- **Loading indicator** â€” Shown during backend processing
- **Results panel** â€” Lists detected patterns with severity badges, descriptions, locations, and fix suggestions
- **Feedback panel** â€” Star rating (1â€“5) + optional comment
- **Code decorations** â€” Inline red highlights on bug lines via `vscode.TextEditorDecorationType`

### 6.2 Analysis Workflow

1. Open any Python file in VS Code
2. Click the **CodeGuard** icon in the Activity Bar (left sidebar)
3. In the sidebar, type the prompt that was used to generate the code
4. Click **"Analyze Entire File"**
5. The panel shows a loading spinner â€” the backend returns stages 1+2 in ~2 seconds
6. After ~100 seconds, the panel automatically updates with full linguistic analysis results
7. Bug lines are highlighted red in the code editor with hover tooltips

### 6.3 API Endpoints

| Endpoint | Method | Description |
|---|---|---|
| `/` | GET | Health / welcome |
| `/health` | GET | Render health probe |
| `/api/analyze` | POST | Submit code for analysis |
| `/api/analysis/{id}` | GET | Poll for analysis status/result |
| `/api/history` | GET | List recent analyses |
| `/api/stats` | GET | Aggregate statistics |
| `/api/feedback` | POST | Submit user rating |
| `/api/patterns` | GET | List all 10 bug patterns |

### 6.4 End-to-End Code Flow

This section traces every function call from the moment the user clicks "Analyze" to the moment red highlights appear in the editor.

---

#### 6.4.1 Full Flow Diagram

```
USER ACTION
  â”‚  Opens a Python file, clicks CodeGuard icon in Activity Bar,
  â”‚  types prompt in sidebar, clicks "Analyze Entire File"
  â”‚
  â–¼
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  VS Code Extension  (out/extension.js â€” esbuild bundle)         â•‘
â•‘                                                                  â•‘
â•‘  1. webview.html                                                 â•‘
â•‘     â””â”€ postMessage({ command: "analyzeCode", data: {prompt} })  â•‘
â•‘          â”‚                                                       â•‘
â•‘  2. SidePanel.ts  â†’  handleAnalyzeRequest()                     â•‘
â•‘     â”œâ”€ editor.document.getText()  â† full file content           â•‘
â•‘     â”œâ”€ this.showLoading()  â† spinner to webview                 â•‘
â•‘     â””â”€ analyzeCode({ prompt, code })  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â•‘
â•‘                                                             â”‚   â•‘
â•‘  3. apiService.ts  â†’  analyzeCode()                         â”‚   â•‘
â•‘     â”œâ”€ reads vscode.workspace.getConfiguration('codeguard') â”‚   â•‘
â•‘     â”‚    â†’ apiUrl (Render) or localhost:8000                â”‚   â•‘
â•‘     â””â”€ axios.POST  /api/analyze  { prompt, code }  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â•‘
â•‘          timeout: 60 000 ms                                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    â”‚  HTTP POST  (JSON body)
                    â–¼
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  FastAPI Backend  (Render.com â€” uvicorn, 1 worker)              â•‘
â•‘                                                                  â•‘
â•‘  4. Rate Limiter  (slowapi)                                      â•‘
â•‘     â””â”€ 30 requests / minute / IP  â†’  429 if exceeded            â•‘
â•‘                                                                  â•‘
â•‘  5. analyze_code()  [main.py:193]                               â•‘
â•‘     â”‚                                                            â•‘
â•‘     â”œâ”€ STAGE 1  StaticAnalyzer(code).analyze()      ~4â€“10 ms   â•‘
â•‘     â”‚   â”œâ”€ ast.parse() + astroid.parse()                        â•‘
â•‘     â”‚   â”œâ”€ 10 checks: Syntax, Hallucinated, Incomplete,         â•‘
â•‘     â”‚   â”‚             Silly Mistake, Missing Corner Case, â€¦     â•‘
â•‘     â”‚   â””â”€ returns  static_results: Dict                        â•‘
â•‘     â”‚                                                            â•‘
â•‘     â”œâ”€ STAGE 2  DynamicAnalyzer(code).analyze()    ~200â€“500 ms  â•‘
â•‘     â”‚   â”œâ”€ Docker available? â†’ run in container                  â•‘
â•‘     â”‚   â”‚   (python:3.10-slim, 128MB, no network, 10s timeout)  â•‘
â•‘     â”‚   â”œâ”€ Docker absent (Render)? â†’ subprocess fallback        â•‘
â•‘     â”‚   â”‚   (dangerous-import check first, then subprocess.run) â•‘
â•‘     â”‚   â””â”€ classifies runtime errors â†’ dynamic_results: Dict    â•‘
â•‘     â”‚                                                            â•‘
â•‘     â”œâ”€ STAGE 3  linguistic_results = STUB (empty zeros)         â•‘
â•‘     â”‚   â””â”€ real linguistic analysis deferred to background task â•‘
â•‘     â”‚                                                            â•‘
â•‘     â”œâ”€ STAGE 4  TaxonomyClassifier(static, dynamic, stub)       â•‘
â•‘     â”‚   â””â”€ .classify() â†’ preliminary bug_patterns_list          â•‘
â•‘     â”‚                                                            â•‘
â•‘     â”œâ”€ STAGE 5  ExplainabilityLayer.generate_summary()          â•‘
â•‘     â”‚   â””â”€ severity label + fix suggestions per pattern         â•‘
â•‘     â”‚                                                            â•‘
â•‘     â”œâ”€ DB SAVE  (SQLAlchemy â†’ PostgreSQL / Supabase)            â•‘
â•‘     â”‚   â”œâ”€ INSERT analyses           â†’ analysis_id = 523        â•‘
â•‘     â”‚   â”œâ”€ INSERT bug_patterns       (preliminary)              â•‘
â•‘     â”‚   â””â”€ INSERT execution_logs     (stages 1, 2, 4)           â•‘
â•‘     â”‚                                                            â•‘
â•‘     â”œâ”€ _processing.add(523)                                      â•‘
â•‘     â”œâ”€ background_tasks.add_task(_run_linguistic_background, â€¦) â•‘
â•‘     â”‚                                                            â•‘
â•‘     â””â”€ RETURN  AnalysisResponse(status="processing", id=523)   â•‘
â•‘          < 2 seconds total >                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    â”‚  HTTP 200  { status: "processing", analysis_id: 523, â€¦ }
                    â–¼
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  apiService.ts  (back in extension)                             â•‘
â•‘                                                                  â•‘
â•‘  6. response.data.status === "processing"                       â•‘
â•‘     â””â”€ pollForCompletion(523, apiUrl)                            â•‘
â•‘          loop: MAX 20 attempts Ã— 15 s = 5 min                   â•‘
â•‘          each attempt: GET /api/analysis/523                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        â”‚  (concurrent, after HTTP response sent)
        â–¼
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  _run_linguistic_background()  [main.py:82]  BACKGROUND TASK    â•‘
â•‘                                                                  â•‘
â•‘  7. LinguisticAnalyzer(prompt, code).analyze()                  â•‘
â•‘     â”‚                                                            â•‘
â•‘     â”œâ”€ NPCDetector.detect()                                      â•‘
â•‘     â”‚   â”œâ”€ Layer 1  RuleEngine.detect_npc()          ~10 ms     â•‘
â•‘     â”‚   â”œâ”€ Layer 2  ASTAnalyzer.verify_npc()         ~50 ms     â•‘
â•‘     â”‚   â””â”€ Layer 3  LLMReasoner.final_verdict()     ~300 ms     â•‘
â•‘     â”‚                â””â”€ POST https://ollama.com/api/chat        â•‘
â•‘     â”‚                   (fallback: OpenRouter /completions)     â•‘
â•‘     â”‚                                                            â•‘
â•‘     â”œâ”€ PromptBiasDetector.detect()    same 3 layers  ~300 ms   â•‘
â•‘     â”œâ”€ MissingFeatureDetector.detect()               ~300 ms   â•‘
â•‘     â””â”€ MisinterpretationDetector.detect()            ~300 ms   â•‘
â•‘                                                                  â•‘
â•‘     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â•‘
â•‘     Total: 4 LLM calls Ã— ~25 s each = ~98â€“105 s                â•‘
â•‘     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â•‘
â•‘                                                                  â•‘
â•‘  8. Re-classify with full results                                â•‘
â•‘     â””â”€ TaxonomyClassifier(static, dynamic, linguistic_results)  â•‘
â•‘          .classify()  â†’  full bug_patterns_list                  â•‘
â•‘                                                                  â•‘
â•‘  9. DB UPDATE                                                    â•‘
â•‘     â”œâ”€ UPDATE analyses SET severity, has_bugs, summary, â€¦       â•‘
â•‘     â”œâ”€ DELETE FROM bug_patterns WHERE analysis_id = 523         â•‘
â•‘     â”œâ”€ INSERT bug_patterns  (full set includes linguistic)       â•‘
â•‘     â””â”€ INSERT linguistic_analyses  (intent_match_score, â€¦)      â•‘
â•‘                                                                  â•‘
â•‘  10. _processing.discard(523)                                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        â”‚
        â”‚  (poll hits endpoint after background task finishes)
        â–¼
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  GET /api/analysis/523  [main.py:488]                           â•‘
â•‘                                                                  â•‘
â•‘  11. status = "processing" if 523 in _processing else "complete"â•‘
â•‘      523 NOT in _processing  â†’  status = "complete"             â•‘
â•‘                                                                  â•‘
â•‘  12. Query DB â†’ full Analysis + BugPattern + LinguisticAnalysis â•‘
â•‘      Return JSON  { status: "complete", bug_patterns: [...], â€¦} â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    â”‚  HTTP 200  { status: "complete", â€¦ }
                    â–¼
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  pollForCompletion()  â†’  returns final result                   â•‘
â•‘                                                                  â•‘
â•‘  13. SidePanel.ts  â†’  handleAnalyzeRequest() resumes            â•‘
â•‘     â”œâ”€ this.showAnalysis(result)                                 â•‘
â•‘     â”‚   â””â”€ webview.postMessage({ command: "showAnalysis",       â•‘
â•‘     â”‚                            data: result })                â•‘
â•‘     â”‚       â†’ webview.html renders bug cards in sidebar         â•‘
â•‘     â”‚                                                            â•‘
â•‘     â”œâ”€ this.addBugDecorations(editor, result.bug_patterns)      â•‘
â•‘     â”‚   â”œâ”€ parse location: "Line 2" â†’ lineNumber = 1 (0-index) â•‘
â•‘     â”‚   â”œâ”€ vscode.TextEditorDecorationType                      â•‘
â•‘     â”‚   â”‚   (red background + red border on bug lines)          â•‘
â•‘     â”‚   â””â”€ editor.setDecorations(decorationType, ranges)        â•‘
â•‘     â”‚                                                            â•‘
â•‘     â””â”€ vscode.window.showWarningMessage(                        â•‘
â•‘            "Found N bug pattern(s) in filename.py")             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        â”‚  (optional)
        â–¼
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  User submits feedback (star rating + comment)                  â•‘
â•‘                                                                  â•‘
â•‘  14. webview.html postMessage({ command: "submitFeedback", â€¦ }) â•‘
â•‘      SidePanel.handleFeedbackSubmission()                        â•‘
â•‘      â””â”€ apiService.submitFeedback()                             â•‘
â•‘           â””â”€ axios.POST  /api/feedback                          â•‘
â•‘                { analysis_id, rating, comment, is_helpful }     â•‘
â•‘                â”‚                                                 â•‘
â•‘                â–¼ Backend                                        â•‘
â•‘           INSERT feedback  (analysis_id FK, rating, comment)    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

#### 6.4.2 Step-by-Step Description

| Step | Where | What Happens |
|:--|:--|:--|
| 1 | `webview.html` | User clicks "Analyze" â€” JS calls `vscode.postMessage({ command: 'analyzeCode', data: { prompt } })` |
| 2 | `SidePanel.ts:handleAnalyzeRequest()` | Reads full file via `editor.document.getText()`, calls `this.showLoading()` to start spinner in webview |
| 3 | `apiService.ts:analyzeCode()` | Reads `codeguard.apiUrl` from VS Code settings; sends `axios.POST /api/analyze` with `{ prompt, code }` and 60 s timeout |
| 4 | `main.py` rate limiter | `slowapi` checks request count â‰¤ 30/min/IP |
| 5 | `StaticAnalyzer.analyze()` | Parses code with `ast` + `astroid`; runs 10 structural checks; returns `static_results` dict in ~4â€“10 ms |
| 6 | `DynamicAnalyzer.analyze()` | Executes code in Docker container (or `subprocess` on Render); classifies `AttributeError`, `TypeError`, `NameError`, `ZeroDivisionError`; returns `dynamic_results` in ~200â€“500 ms |
| 7 | `TaxonomyClassifier.classify()` | Maps static + dynamic results to the 10-pattern taxonomy; `linguistic_results` is a zero-filled stub at this point |
| 8 | `ExplainabilityLayer.generate_summary()` | Produces human-readable description + fix suggestions for each detected pattern |
| 9 | DB save | `INSERT` into `analyses`, `bug_patterns`, `execution_logs` via SQLAlchemy; `db.flush()` to get `analysis_id` before `commit` |
| 10 | Background task scheduled | `_processing.add(analysis_id)`; `background_tasks.add_task(_run_linguistic_background, â€¦)` â€” fires after HTTP response is sent |
| 11 | HTTP response sent | `AnalysisResponse(status="processing", analysis_id=523, â€¦)` returned in **< 2 seconds** |
| 12 | `apiService.ts:pollForCompletion()` | Detects `status === "processing"`; enters poll loop â€” `GET /api/analysis/523` every 15 s, up to 20 attempts (5 min) |
| 13 | `_run_linguistic_background()` | Runs in Starlette background thread: `LinguisticAnalyzer(prompt, code).analyze()` â€” 4 detectors Ã— 3 layers each |
| 14 | Each detector Layer 3 | `LLMReasoner.final_verdict()` constructs a structured prompt from Layer 1 + Layer 2 evidence and sends it to `POST https://ollama.com/api/chat` (primary) or OpenRouter `/completions` (fallback); LLM returns JSON verdict + severity |
| 15 | DB update | Full `bug_patterns` set replaces preliminary rows; `linguistic_analyses` record inserted; `_processing.discard(analysis_id)` called |
| 16 | Poll returns `"complete"` | `GET /api/analysis/523` checks `523 not in _processing` â†’ `status = "complete"`; full result including `linguistic_analysis` field returned |
| 17 | `SidePanel.showAnalysis(result)` | `webview.postMessage({ command: "showAnalysis", data: result })` â€” sidebar renders bug cards with pattern name, severity, description, and fix suggestions |
| 18 | `addBugDecorations()` | Parses `"Line N"` from each bug's `location` field; applies `vscode.TextEditorDecorationType` (red background + red border) to those lines in the active editor |
| 19 | Notification | `vscode.window.showWarningMessage("Found N bug pattern(s) in file.py")` appears in the bottom-right corner |
| 20 | Feedback (optional) | User submits star rating â†’ `POST /api/feedback` â†’ `INSERT feedback` row linked to `analysis_id` |

---

#### 6.4.3 Timing Summary

```
 0s â”€â”€â–º User clicks "Analyze"
 0â€“2s â”€â–º Stages 1+2+4+5: Static, Dynamic, Preliminary Classification, DB save
 2s â”€â”€â–º Extension receives { status: "processing" }  â† sidebar shows spinner
 2s â”€â”€â–º Background task starts on server (linguisitc stage)
17s â”€â”€â–º First poll: GET /api/analysis/523  â†’ still "processing"
32s â”€â”€â–º Second poll: still "processing"
...
~102s â–º Background task finishes (_processing.discard)
~107s â–º Next poll: GET returns { status: "complete", full results }
~107s â–º Sidebar renders full bug cards, editor shows red highlights
```

---

## 7. Test Plan & Results

### 7.1 Testing Strategy

- **160 total test cases** across 10 test sets
- Each set: 8 buggy + 8 clean code samples (16 cases per set)
- Ground truth manually labeled
- Tests run as HTTP POST requests to Render production backend
- Binary classification: Bug (`has_bugs=True`) vs. Clean (`has_bugs=False`)

### 7.2 Evaluation Metrics

$$\text{Precision} = \frac{TP}{TP + FP}$$

$$\text{Recall} = \frac{TP}{TP + FN}$$

$$F_1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$$

$$\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}$$

### 7.3 Confusion Matrix

|  | Predicted Bug | Predicted Clean |
|:--|:--|:--|
| **Actual Bug** | TP = 70 | FN = 10 |
| **Actual Clean** | FP = 33 | TN = 47 |

### 7.4 Overall Metrics

| Metric | Baseline (stdlib ast) | Post-Fix (astroid + Docker) | Change |
|:--|:--|:--|:--|
| Accuracy | 71.88% | **73.12%** | +1.25% â†‘ |
| Precision | 68.42% | 67.96% | -0.46% |
| Recall | 81.25% | **87.50%** | +6.25% â†‘ |
| F1 Score | 74.29% | **76.50%** | +2.21% â†‘ |
| Specificity | 62.50% | 58.75% | -3.75% |
| False Negative Rate | 18.75% | **12.50%** | -6.25% â†“ |

### 7.5 Per-Test-Set Results

| Test Set | Name | Cases | Correct | Accuracy | Î” vs Baseline |
|:--|:--|:--|:--|:--|:--|
| Set 1 | Basic Bug Patterns | 16 | 15 | 93.75% | +6.25% â†‘ |
| Set 2 | Advanced Bug Patterns | 16 | 12 | 75.00% | +6.25% â†‘ |
| Set 3 | Real-World Code Scenarios | 16 | 13 | 81.25% | = |
| Set 4 | Data Structures & API Usage | 16 | 12 | 75.00% | +6.25% â†‘ |
| Set 5 | Complex & Real-World Scenarios | 16 | 12 | 75.00% | = |
| Set 6 | Mixed Bugs & Complex Logic | 16 | 11 | 68.75% | -6.25% â†“ |
| Set 7 | Security & Edge Cases | 16 | 11 | 68.75% | = |
| Set 8 | OOP & Structural Bugs | 16 | 11 | 68.75% | -6.25% â†“ |
| Set 9 | Regression & Stress Testing | 16 | 10 | 62.50% | = |
| Set 10 | Production-Ready Code Patterns | 16 | 10 | 62.50% | +6.25% â†‘ |
| **Total** | â€” | **160** | **117** | **73.12%** | **+1.25%** |

### 7.6 Per-Test-Set Confusion Matrix

| Set | TP | TN | FP | FN |
|:--|:--|:--|:--|:--|
| Set 1 | 8 | 7 | 1 | 0 |
| Set 2 | 7 | 5 | 3 | 1 |
| Set 3 | 7 | 6 | 2 | 1 |
| Set 4 | 7 | 5 | 3 | 1 |
| Set 5 | 7 | 5 | 3 | 1 |
| Set 6 | 6 | 5 | 3 | 2 |
| Set 7 | 7 | 4 | 4 | 1 |
| Set 8 | 6 | 5 | 3 | 2 |
| Set 9 | 6 | 4 | 4 | 2 |
| Set 10 | 9 | 1 | 7 | 0 |
| **Total** | **70** | **47** | **33** | **10** |

### 7.7 False Positive Analysis

The main sources of false positives are:
1. **OOP patterns** (Sets 6, 8) â€” `super().__init__()`, property setters, and abstract methods trigger dynamic analysis signals despite being valid Python
2. **Async/await patterns** â€” The dynamic sandbox sometimes fails on `async def` without an event loop, producing an error signal on clean code
3. **`self.name` attribute access** â€” Occasionally flagged as Wrong Attribute when the static detector cannot infer the parent class type

**Mitigation applied:** The astroid migration introduced semantic type inference via `node.expr.infer()` which eliminated most dictionary false positives. A severity threshold of â‰¥4 is recommended to further reduce FPR.

### 7.8 Performance Benchmarks

| Stage | Location | Average Time |
|:--|:--|:--|
| Static Analysis | Render server | ~4â€“10 ms |
| Dynamic Analysis | Render subprocess | ~200â€“500 ms |
| Classification + Explainability | Render server | ~5 ms |
| Initial response (stages 1+2+4) | End-to-end | **< 2 seconds** |
| Linguistic Analysis (background) | Ollama API Ã— 4 calls | **~98â€“105 seconds** |
| Total time to full result | End-to-end | **~100 seconds** |

---

## 8. Limitations & Future Work

### 8.1 Current Limitations

1. **Python Only** â€” The AST parser, dynamic executor, and pattern detectors are all Python-specific. JavaScript, Java, and TypeScript are not supported.
2. **Docker Unavailable on Render** â€” The Docker sandbox cannot run on Render's free tier. The subprocess fallback provides reduced isolation and refuses to execute code with system-level imports.
3. **No User Authentication** â€” The API is publicly accessible with rate limiting only (30 requests/minute/IP). No user accounts or session management.
4. **OOP False Positives** â€” Complex Python OOP patterns (multiple inheritance, property decorators, abstract base classes) are occasionally misclassified.
5. **Linguistic Analysis Latency** â€” Four sequential LLM API calls produce a ~100-second wait per analysis. The background task pattern makes this transparent to the user but the final result is still delayed.
6. **Single Render Instance** â€” Render free tier runs one worker with no persistent memory. The `_processing` set (tracking in-flight background tasks) is lost on restart, potentially causing stale `"processing"` status.

### 8.2 Future Improvements

1. **Parallel Linguistic Detectors** â€” Run the four linguistic detectors concurrently using `asyncio.gather()` to reduce Stage 3 from ~100s to ~25â€“30s (single LLM call latency)
2. **Multi-language Support** â€” Tree-sitter for JavaScript/TypeScript AST, enabling the same taxonomy on JS/TS codebases
3. **Feedback-Driven Learning** â€” Use the 1â€“5 star ratings stored in the `feedback` table to fine-tune pattern thresholds and reduce false positives
4. **Authentication** â€” JWT-based multi-user support with per-user analysis history
5. **IDE Integrations** â€” JetBrains plugin (IntelliJ IDEA, PyCharm) and Neovim LSP integration
6. **Persistent Background Task State** â€” Replace the in-memory `_processing` set with a Redis-backed queue to survive server restarts
7. **Severity Calibration** â€” Raise the bug/clean threshold from `severity > 0` to `severity >= 4` to reduce FPR at marginal recall cost

---

## 9. Conclusion

CodeGuard demonstrates that LLM-generated code requires a dedicated, multi-modal analysis approach. Traditional static linters detect syntax and style violations but are blind to the semantic gap between developer intent and model output. This project makes three concrete contributions:

1. **A 10-pattern taxonomy of LLM-generated bugs**, the first systematic classification of failure modes specific to AI code generation, covering static errors (Syntax, Hallucination, Incomplete, Silly Mistake), runtime errors (Wrong Attribute, Wrong Type, Missing Corner Case), and semantic misalignments (NPC, Prompt Bias, Misinterpretation)

2. **A three-stage hybrid detection pipeline** combining AST-based static analysis (migrated to astroid for semantic type inference), Docker-sandboxed dynamic execution (with subprocess fallback for cloud deployment), and intent-aware linguistic analysis backed by an LLM verdict layer

3. **A production VS Code extension** that integrates this pipeline into the developer workflow with real-time sidebar feedback, inline code highlights, and a background-task architecture that returns preliminary results in under 2 seconds

Evaluated across 160 labeled test cases, the system achieves 73.12% accuracy, 87.50% recall, and an F1 score of 76.50%. The recall improvement from 81.25% to 87.50% after enabling Docker dynamic execution validates the hypothesis that runtime signals are essential for catching LLM-specific bugs that no static analyzer can detect. CodeGuard is a working proof of concept that intent-aware, multi-stage analysis is the correct approach for the LLM code quality problem.

---

## 10. References

1. Pearce, H., Ahmad, B., Tan, B., Dolan-Gavitt, B., & Karri, R. (2022). *Asleep at the Keyboard? Assessing the Security of GitHub Copilot's Code Contributions.* IEEE S&P.
2. Liu, J., Xia, C. S., Wang, Y., & Zhang, L. (2023). *Is Your Code Generated by ChatGPT Really Correct?* arXiv:2305.01210.
3. GitHub. (2023). *GitHub Copilot: The AI pair programmer.* [Usage Statistics Report].
4. Chen, M., et al. (2021). *Evaluating Large Language Models Trained on Code (Codex).* arXiv:2107.03374.
5. Louridas, P. (2006). *Static code analysis.* IEEE Software, 23(4), 58â€“61.
6. Merkel, D. (2014). *Docker: lightweight Linux containers for consistent development and deployment.* Linux Journal, 239.
7. Salton, G., & Buckley, C. (1988). *Term-weighting approaches in automatic text retrieval.* Information Processing & Management, 24(5), 513â€“523.
8. Microsoft. (2023). *VS Code Extension API.* https://code.visualstudio.com/api
9. Tiobe Software. (2024). *TIOBE Index for Python.*
10. OpenAI. (2023). *GPT-4 Technical Report.* arXiv:2303.08774.

---

## 11. Appendix

### Appendix A â€” Installation & Setup Guide

**Prerequisites:** Node.js 18+, Python 3.10+, Docker Desktop (optional)

**1. Install the VS Code Extension**
```
Extensions â†’ â‹¯ â†’ Install from VSIX
â†’ codeguard-llm-bugs-classifier-0.0.7.vsix
```

**2. Configure Backend URL** (VS Code Settings)
```json
"codeguard.apiUrl": "https://codeguard-backend-g7ka.onrender.com"
```

**3. Run the Backend Locally (optional)**
```bash
cd backend
pip install -r requirements-light.txt
uvicorn app.main:app --reload --port 8000
```
Then set `"codeguard.useLocalBackend": true` in VS Code settings.

---

### Appendix B â€” Sample API Request & Response

**Request:**
```http
POST /api/analyze
Content-Type: application/json

{
  "prompt": "Write a function that divides two numbers",
  "code": "def divide(a, b):\n    return a / b"
}
```

**Response (immediate, ~2s):**
```json
{
  "analysis_id": 523,
  "status": "processing",
  "has_bugs": true,
  "overall_severity": 9,
  "bug_patterns": [
    {
      "pattern_name": "Missing Corner Case",
      "severity": 6,
      "confidence": 0.85,
      "description": "Division without zero-check: b=0 raises ZeroDivisionError",
      "location": "Line 2",
      "fix_suggestion": "Add: if b == 0: raise ValueError('Division by zero')",
      "detection_stage": "static"
    }
  ],
  "summary": "1 bug pattern detected: Missing Corner Case (severity 6/10)"
}
```

**Poll Response (after ~100s):**
```json
{
  "analysis_id": 523,
  "status": "complete",
  "has_bugs": true,
  "overall_severity": 9,
  "bug_patterns": [ ... 6 patterns including linguistic results ... ],
  "linguistic_analysis": {
    "intent_match_score": 0.72,
    "missing_features": ["zero_check", "validation"],
    "unprompted_features": [],
    "hardcoded_values": []
  }
}
```

---

### Appendix C â€” Environment Variables

| Variable | Description | Example |
|---|---|---|
| `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host/db` |
| `OLLAMA_API_KEY` | Ollama Cloud API key | `sk-...` |
| `OPENROUTER_API_KEY` | OpenRouter fallback API key | `sk-or-...` |
| `DOCKER_HOST` | Docker socket path | `unix:///var/run/docker.sock` |
| `DOCKER_TIMEOUT` | Container execution timeout (s) | `10` |
| `DOCKER_MEMORY_LIMIT` | Container memory cap | `128m` |
| `ENVIRONMENT` | `development` enables CORS `*` | `production` |

---

### Appendix D â€” Project File Structure

```
Codeguard/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI app, 3-stage pipeline, background tasks
â”‚   â”‚   â”œâ”€â”€ models.py            # SQLAlchemy ORM models (5 tables)
â”‚   â”‚   â”œâ”€â”€ schemas.py           # Pydantic request/response schemas
â”‚   â”‚   â”œâ”€â”€ database.py          # DB engine, session factory
â”‚   â”‚   â””â”€â”€ analyzers/
â”‚   â”‚       â”œâ”€â”€ static_analyzer.py      # Stage 1: AST-based 10-check analysis
â”‚   â”‚       â”œâ”€â”€ dynamic_analyzer.py     # Stage 2: Docker + subprocess execution
â”‚   â”‚       â”œâ”€â”€ linguistic_analyzer.py  # Stage 3: 4-detector intent analysis
â”‚   â”‚       â”œâ”€â”€ classifier.py           # Taxonomy mapping (all 3 stages â†’ 10 patterns)
â”‚   â”‚       â”œâ”€â”€ explainer.py            # Human-readable summaries + fix suggestions
â”‚   â”‚       â”œâ”€â”€ linguistic/
â”‚   â”‚       â”‚   â”œâ”€â”€ LLM_response.py          # Ollama/OpenRouter dual-API wrapper
â”‚   â”‚       â”‚   â”œâ”€â”€ npc_detector.py          # Non-Prompted Consideration detector
â”‚   â”‚       â”‚   â”œâ”€â”€ prompt_bias_detector.py  # Prompt-Biased Code detector
â”‚   â”‚       â”‚   â”œâ”€â”€ missing_feature_detector.py  # Missing Feature / Intent gap
â”‚   â”‚       â”‚   â”œâ”€â”€ misinterpretation_detector.py  # Semantic mismatch detector
â”‚   â”‚       â”‚   â””â”€â”€ layers/
â”‚   â”‚       â”‚       â””â”€â”€ layer3_llm_reasoner.py  # LLM verdict layer
â”‚   â”‚       â””â”€â”€ static/
â”‚   â”‚           â””â”€â”€ detectors/           # Astroid-based individual detectors
â”‚   â”œâ”€â”€ requirements-light.txt   # Production dependencies
â”‚   â””â”€â”€ render.yaml              # Render deployment config
â”œâ”€â”€ codeguard-vscode-extension/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ extension.ts         # VS Code activation + command registration
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ apiService.ts    # axios POST + polling logic
â”‚   â”‚   â””â”€â”€ webview/
â”‚   â”‚       â”œâ”€â”€ SidePanel.ts     # WebviewViewProvider implementation
â”‚   â”‚       â””â”€â”€ webview.html     # Sidebar panel UI (HTML/CSS/JS)
â”‚   â”œâ”€â”€ esbuild.js               # Bundle script (axios baked in)
â”‚   â””â”€â”€ package.json             # Extension manifest + contributes
â””â”€â”€ README.md                    # This file
```
